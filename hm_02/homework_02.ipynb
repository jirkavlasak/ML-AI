# Úkol č. 2 - Využití neuronových sítí

  * **Deadline je 13. 5. 2024, 23:59:59**, pokud odevzdáte úkol do 20. 5. 2024, 23:59:59, budete penalizování -12 body, pozdější odevzdání je bez bodu.
  * V rámci tohoto úkolu musíte sestrojit vhodný model neuronové sítě pro vícetřídou klasifikaci.
  * Část bodů získáte za správné vypracování a část bodů získáte za výslednou přesnost Vašeho modelu na evaluačních datech.
    
> **Úkoly jsou zadány tak, aby Vám daly prostor pro invenci. Vymyslet _jak přesně_ budete úkol řešit, je důležitou součástí zadání a originalita či nápaditost bude také hodnocena!**

Využívejte buňky typu `Markdown` k vysvětlování Vašeho postupu. Za nepřehlednost budou strhávány body.

## Zdroj dat

 * Zdrojem dat jsou soubory `train.csv` a `evaluate.csv`.
 * Jedná se o obrázky 32x32 pixelů ve stupních šedi, které byly nějakým způsobem vyrobeny z [Fashion Mnist datasetu](https://www.kaggle.com/datasets/zalando-research/fashionmnist).
 * Soubor `train.csv` obsahuje trénovací data.
 * Cílová (vysvětlovaná) proměnná se jmenuje **label**.
 * Soubor `evaluate.csv` obsahuje testovací data bez hodnot skutečných labelů.

## Pokyny k vypracování (max 18 bodů)

**Body zadání**, za jejichž (poctivé) vypracování získáte **18 bodů**:
  * V notebooku načtěte data ze souboru `train.csv`. Vhodným způsobem si je rozdělte na podmnožiny, které Vám poslouží pro trénování, porovnávání modelů a následnou predikci výkonnosti finálního modelu.
  * Proveďte základní průzkum dat a svá pozorování diskutujte. Některé obrázky také zobrazte.
  * Sestrojte a natrénujte několik variant modelu dopředné neuronové sítě. Přitom v rámci výpočetních možností:
      * Okomentujte vhodnost daného modelu pro daný typ úlohy.
      * Experimentujte s různými hloubkami a velikosmi vrstev.
      * Experimentujte se standardizací/normalizací dat.
      * Experimentujte s různými optimalizačními metodami.
      * Experimentujte s různými regularizačními technikami.
      * Získané výsledky vždy řádně okomentujte.

  * Sestrojte model konvoluční neuronové sítě. Přitom v rámci výpočetních možností:
      * Okomentujte vhodnost daného modelu pro daný typ úlohy.
      * Experimentujte s různými hloubkami a velikosmi vrstev.
      * Experimentujte se standardizací/normalizací dat.
      * Experimentujte s různými optimalizačními metodami.
      * Experimentujte s různými regularizačními technikami.
      * Získané výsledky vždy řádně okomentujte.
    
  * Ze všech zkoušených možností vyberte finální model a odhadněte, jakou přesnost můžete očekávat na nových datech, která jste doposud neměli k dispozici.
  
  * Nakonec načtěte vyhodnocovací data ze souboru`evaluate.csv`. Pomocí finálního modelu napočítejte predikce pro tyto data (vysvětlovaná proměnná v nich již není). Vytvořte soubor `results.csv`, ve kterém získané predikce uložíte do sloupce **label** a identifikátory do sloupce **ID**. Tento soubor též odevzdejte (uložte do projektu vedle notebooku).
   
   * Ukázka prvních řádků souboru `results.csv`:
  
```
ID,label
0,0
1,1
...
```

## Vyhodnocovací část (max 7 bodů)
Za přesnost (accuraccy) na odevzdaných predikcích pro vyhodnocovací množnu získáte dalších max **7 bodů**.

Označíme-li $A$ přesnost, které jste dosáhli, zaokrouhlenou na 2 desetinná místa, akumulují se výsledné body podle následujících pravidel:
* pokud $A \geq 0.80$ obdržíte +1 bod
* pokud $A \geq 0.83$ obdržíte +1 bod
* pokud $A \geq 0.86$ obdržíte +1 bod
* pokud $A \geq 0.87$ obdržíte +1 bod
* pokud $A \geq 0.88$ obdržíte +1 bod
* pokud $A \geq 0.89$ obdržíte +1 bod
* pokud $A \geq 0.90$ obdržíte +1 bod

**Příklad:** Pokud bude Vaše přesnost 0.856, vyjde A = 0.86 a vy získáte 3 body.


## Poznámky k odevzdání

  * Řiďte se pokyny ze stránky https://courses.fit.cvut.cz/BI-ML2/homeworks/index.html.
  * Vytvořte i csv soubor `results.csv` s predikcemi a uložte ho v rámci projektu vedle ipython notebooku.
### Odtud uz je to vase
Nejprve si nactu data a udelam zakladni prehled, abych vedel vlastne s jakymi daty budu pracovat.

import numpy as np
import pandas as pd
data_frame = pd.read_csv('train.csv')
data_frame.info()
data_frame.head()
uniq_label =data_frame['label'].unique().tolist()
uniq_label

print(len(data_frame))
print(data_frame.shape)
print(data_frame.mean())
import matplotlib.pyplot as plt
data_frame['label'].hist()
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Histogram label')
plt.show()
Provedl jsem zakladni praci pro prehled dat, dale jsem si vykreslil zastoupeni jednotlivych "labels" a  muzeme usoudit, ze zastoupeni jednotlivych labels je dosti stejne.
Ted si vykreslime nekolik obrazku, abychom valstne vedeli s cim pracujeme.
data_frame_np = data_frame.drop(columns=['label']).to_numpy()

import random

num_img = 20
random_index = random.sample(range(len(data_frame_np)), num_img)

plt.figure(figsize=(12, 6))
for i, idx in enumerate(random_index):
    plt.subplot(4, 5, i+1)
    plt.imshow(data_frame_np[idx].reshape((32, 32)), cmap='gray')
    plt.axis('off')
    original_label = data_frame.loc[idx, 'label']
    plt.title(f'Image {idx}\nLabel: {original_label}')

plt.tight_layout()
plt.show()
Zacneme s doprednou neuronovou siti. Jsou casto pouzivane pro klasifikaci a a regresi, jsou velmi dobre schopne zachytit slozite vztahy mezi vstupnimi a vystupnimi daty.
Rozdelime si data se kterymi budeme dale pracovat
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import transforms

data_frame_numpy = data_frame.to_numpy()

generator1 = torch.Generator().manual_seed(42)
train_size = int(0.6*len(data_frame_numpy))
val_size = int(0.2*len(data_frame_numpy))
test_size = len(data_frame_numpy) - train_size - val_size

train_data, val_data, test_data = torch.utils.data.random_split(data_frame_numpy, [train_size, val_size, test_size],generator1)

print("Velikost trénovací množiny:", len(train_data))
print("Velikost validační množiny:", len(val_data))
print("Velikost testovací množiny:", len(test_data))

Dale si pripravim "davky" pro davkove uceni
class CustomDataset(Dataset):
    def __init__(self, dataset, indices):
        self.data = torch.from_numpy(dataset[indices, :-1]).float()  # Features
        self.labels = torch.from_numpy(dataset[indices, -1]).long()  # Labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]
train_dataset = CustomDataset(data_frame_numpy, train_data.indices)
val_dataset = CustomDataset(data_frame_numpy, val_data.indices)
test_dataset = CustomDataset(data_frame_numpy, test_data.indices)

batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
Ted si pripravime nejprve jednoduchy model dopredne neuronove site 
class firstNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(firstNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = x.view(x.size(0), -1)  
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x
def run_model(model, criterion, optimizer, train_loader, val_loader, test_loader):
    num_epochs = 10
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
        epoch_loss = running_loss / len(train_loader.dataset)

        model.eval()
        val_running_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                val_running_loss += loss.item() * inputs.size(0)
        val_epoch_loss = val_running_loss / len(val_loader.dataset)
        val_accuracy = correct / total

        print(f'Epoch [{epoch+1}/{num_epochs}], '
              f'Training Loss: {epoch_loss:.4f}, '
              f'Validation Loss: {val_epoch_loss:.4f}, '
              f'Validation Accuracy: {val_accuracy:.4f}')

    model.eval()
    test_running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            test_running_loss += loss.item() * inputs.size(0)
    test_loss = test_running_loss / len(test_loader.dataset)
    test_accuracy = correct / total

    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')
model_one = firstNN(input_size=32*32, hidden_size=228, output_size=10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_one.parameters(), lr=0.0001)
run_model(model_one,criterion,optimizer,train_loader,val_loader,test_loader)
U tohoto modelu jsem experimentoval nejvice s hodnotou "lr" a prave lr = 0.0001 mi vysla jako nejvice optimalni.Pro odlisne (mensi jak 128) hloubky skrytych hloubbek jsem dostaval mensi presnost, pro hodnoty vetsi jak 128 se presnost zvetsovala ale od 128 je lehce, proto nechavam u tohoto modelu 228. Vstup je 1024 coz odpovida pixelum u obrazku a vystupni vrstva ma 10. Jeste vyzkouskim jinym pristup u dopredne neuronovou site. U tohoto modelu mi presnost vicemen vychazi okolo 80%, pokusime se udelat lepsi model. 
class secondCNN(nn.Module):
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):
        super(secondCNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size1)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(hidden_size2, output_size)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        x = self.relu2(x)
        x = self.fc3(x)
        return x
model_two = secondCNN(input_size=32*32, hidden_size1=556,hidden_size2=228, output_size=10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_one.parameters(), lr=0.00001)
run_model(model_one,criterion,optimizer,train_loader,val_loader,test_loader)
Zde jsem take experimentoval nejvice s hodnotou "lr", hodnota pro lr- 0.00001 mi davala nejlepsi vysledky pro model a odhad je okolo 82%. Zde jsem opet zkousel nekolik ruznych variant pro skryte vrstvy a nejlepe jsem dostal na hodnotach 556 a 258 s presnosti 83%.
Uvedu zde jeste treti model
class ThirdNN(nn.Module):
    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):
        super(ThirdNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size1)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(hidden_size2, hidden_size3)
        self.relu3 = nn.ReLU()
        self.fc4 = nn.Linear(hidden_size3, output_size)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        x = self.relu2(x)
        x = self.fc3(x)
        x = self.relu3(x)
        x = self.fc4(x)
        return x
model_three = ThirdNN(input_size=32*32, hidden_size1=556,hidden_size2=228,hidden_size3=228, output_size=10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_one.parameters(), lr=0.00001)
run_model(model_one,criterion,optimizer,train_loader,val_loader,test_loader)
U teto metody jsem dostal o neco vetsi presnost, ale testovaci chyba byla vetsi nez u predchoziho modelu. Zkousel jsem ruzne velikosti skrytych vrstev, ale tyto parametry mi vysly jako nejlepsi.
Prejdeme ted na konvolucni neronovo sit, CNN jsou velmi schopne pro odhaleni lekolnich vzoru diky lokalnim filtrum. Pouzivaji se momentalne hojne, protoze jsou schopny zachytit hieararchicke struktury vizualnich dat. Jsou casto pouzite pro rozpoznavani objektu/obrazu.
import torch
from sklearn.model_selection import train_test_split

data_np = data_frame.to_numpy()


features_np = data_np[:, :-1]  
labels_np = data_np[:, -1]     


features_tensor = torch.tensor(features_np, dtype=torch.float32)
labels_tensor = torch.tensor(labels_np, dtype=torch.long)


batch_size = 32 
channels = 1  
height = 32
width = 32


features_tensor = features_tensor.view(-1, channels, height, width)


X_train_val, X_test, y_train_val, y_test = train_test_split(features_tensor, labels_tensor, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2


from torch.utils.data import DataLoader, TensorDataset

train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

val_dataset = TensorDataset(X_val, y_val)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

test_dataset = TensorDataset(X_test, y_test)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


print("Velikost trénovacích dat:", len(train_loader.dataset))
print("Velikost validačních dat:", len(val_loader.dataset))
print("Velikost testovacích dat:", len(test_loader.dataset))

import torch.nn as nn
import torch.nn.functional as F

class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 8 * 8, 128)  
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)  
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
Sestrojim si funkci podobnou jako predtim na testovani dat.
def run_model_two(model, criterion, optimizer, train_loader, val_loader, test_loader, num_epochs=10):
    for epoch in range(num_epochs):

        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
        epoch_loss = running_loss / len(train_loader.dataset)


        model.eval()
        val_running_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                val_running_loss += loss.item() * inputs.size(0)
        val_epoch_loss = val_running_loss / len(val_loader.dataset)
        val_accuracy = correct / total

        print(f'Epoch [{epoch+1}/{num_epochs}], '
              f'Training Loss: {epoch_loss:.4f}, '
              f'Validation Loss: {val_epoch_loss:.4f}, '
              f'Validation Accuracy: {val_accuracy:.4f}')

 
    model.eval()
    test_running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            test_running_loss += loss.item() * inputs.size(0)
    test_loss = test_running_loss / len(test_loader.dataset)
    test_accuracy = correct / total

    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')
model = ConvNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

run_model_two(model, criterion, optimizer, train_loader, val_loader, test_loader)
U tohoto modelu jsem experimentoval s nekolika zakldnimi funkcemi a modely dat, tato kombinace mi vysla jako nejvice efektivni a ocekvany vysledek 88% presnosti je velmi uspokojujici., tento model pouziji na finalni data a pro jejich odhad. Ocekavam presnost okolo 88%

Dale si model aplikuji na finalni data ty potom nahraji do finalniho csv souboru.
final_data  = pd.read_csv('evaluate.csv')
final_data_np = data_frame.to_numpy()


features_np = data_np[:, :-1]
labels_np = data_np[:, -1]


features_tensor = torch.tensor(features_np, dtype=torch.float32)



batch_size = 32
channels = 1
height = 32
width = 32


features_tensor = features_tensor.view(-1, channels, height, width)

model.eval()
with torch.no_grad():
    predicts = model(features_tensor)

predicted_classes = torch.argmax(predicts,dim=1)
print(predicted_classes)
import csv

predicted_classes = [label.item() for label in predicted_classes]
ids = list(range(len(predicted_classes)))
results = list(zip(ids, predicted_classes))

with open('results.csv','w',newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['ID','label'])
    writer.writerows(results)
